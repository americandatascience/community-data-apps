{
  "cells": [
    {
      "source": "# Test AI Tinkerers Demo: Streaming and Async Calls with LLMs",
      "outputs": [
        {
          "data": {
            "text/plain": "# Test AI Tinkerers Demo: Streaming and Async Calls with LLMs",
            "text/markdown": "# Test AI Tinkerers Demo: Streaming and Async Calls with LLMs"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {
        "id": "d79e34f0-a17f-401f-892a-59e52af943a8"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "completed",
      "execution_count": null,
      "executionEndTime": "2026-02-25T15:34:18.100Z",
      "executionStartTime": "2026-02-25T15:34:18.100Z"
    },
    {
      "source": "## 0. Load env vars and set client",
      "outputs": [],
      "metadata": {
        "id": "80434a7c-1fb7-4803-9ee5-22154de91819"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "from ai21 import AI21Client\nfrom ai21.models.chat import ChatMessage\nimport dotenv\nimport time\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Set up the AI21 client\nclient = AI21Client()\n\nmodel = \"jamba-mini-1.6-2025-03\"",
      "outputs": [],
      "metadata": {
        "id": "cfbcbc08-0118-42f1-b735-3f9ffeadc7ff",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": 5,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "## 1. Typical calls to API LLMs",
      "outputs": [],
      "metadata": {
        "id": "be4648c9-866e-46b5-9205-465beb4ba0e0"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Define the system and user messages\nsystem = \"You're a data feature enrichment engineer.\"\nmessages = [\n    ChatMessage(content=system, role=\"system\"),\n    ChatMessage(content=\"Hello, I need help with enriching CSVs.\", role=\"user\"),\n]\n\n# Start timing the request\nstart_time = time.time()\n\n# Make the API call\nchat_completions = client.chat.completions.create(\n    messages=messages,\n    model=model,\n)\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\n\n# Print the chat completion response\nprint(\"Chat Completion Response:\")\nprint(\"-\" * 50)\nprint(f\"Role: {chat_completions.choices[0].message.role}\")\nprint(f\"Content: {chat_completions.choices[0].message.content}\")\nprint(\"-\" * 50)\n\n# Display additional metadata about the response\n# Using dir() and vars() to safely inspect the object structure\nprint(\"\\nResponse Metadata:\")\nprint(f\"Request Time: {elapsed_time:.2f} seconds\")\n\n# Safely access usage information\nif hasattr(chat_completions, 'usage'):\n    usage = chat_completions.usage\n    print(f\"Usage - Prompt Tokens: {usage.prompt_tokens}\")\n    print(f\"Usage - Completion Tokens: {usage.completion_tokens}\")\n    print(f\"Usage - Total Tokens: {usage.total_tokens}\")\n\n# Print the model name from the request rather than trying to access it from the response\nprint(f\"Model: jamba-mini-1.6-2025-03\")\n\n# Print the full response structure for debugging (optional)\nprint(\"\\nFull Response Structure:\")\nprint(f\"Response type: {type(chat_completions)}\")",
      "outputs": [],
      "metadata": {
        "id": "dd8d47d1-8210-4052-b0ef-0b47ba042886",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "### Now use this to loop through rows in drug_reviews.csv to extract \"Positive, Negative\" given the text in \"review\" column",
      "outputs": [],
      "metadata": {
        "id": "12105379-f6cf-4056-b775-80d9ce5a1768"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Import necessary libraries for data processing and sentiment analysis\nimport pandas as pd\nimport time\nfrom ai21 import AI21Client\nfrom ai21.models.chat import ChatMessage\nimport csv\nfrom tqdm import tqdm\n\n# Load the drug reviews dataset\ntry:\n    # Try to load the first few rows to understand the structure\n    df = pd.read_csv('drug_reviews.csv', nrows=5)\n    print(f\"Successfully loaded sample from drug_reviews.csv\")\n    print(f\"Columns: {df.columns.tolist()}\")\n    \n    # Load the full dataset or a subset for processing\n    # Limiting to 20 rows for demonstration purposes\n    # Remove the nrows parameter to process the entire file\n    df = pd.read_csv('drug_reviews.csv', nrows=20)\n    print(f\"\\nLoaded {len(df)} rows for processing\")\n    \n    # Create a new column to store sentiment analysis results\n    df['sentiment'] = \"\"\n    \n    # Set up the AI21 client (using the client already initialized above)\n    client = AI21Client()\n    model = \"jamba-mini-1.6-2025-03\"\n    \n    # Define the system prompt for sentiment analysis\n    system_prompt = \"You are a sentiment analysis expert. Analyze the drug review and respond 'Positive' or 'Negative' with reasoning.\"\n    \n    # Process each review and extract sentiment\n    print(\"\\nProcessing reviews for sentiment analysis...\")\n    \n    # Track time for performance analysis\n    start_time = time.time()\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        # Create messages for the API call\n        messages = [\n            ChatMessage(content=system_prompt, role=\"system\"),\n            ChatMessage(content=f\"Analyze the sentiment of this drug review: {row['review']}\", role=\"user\"),\n        ]\n        \n        # Make the API call\n        try:\n            response = client.chat.completions.create(\n                messages=messages,\n                model=model,\n            )\n            \n            # Extract the sentiment from the response\n            sentiment = response.choices[0].message.content.strip()\n            \n            # Store the sentiment in the dataframe\n            df.at[idx, 'sentiment'] = sentiment\n            \n            # Add a small delay to avoid rate limiting (if needed)\n            time.sleep(0.1)\n            \n        except Exception as e:\n            print(f\"Error processing row {idx}: {str(e)}\")\n            df.at[idx, 'sentiment'] = \"Error\"\n    \n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n    print(f\"\\nProcessing completed in {elapsed_time:.2f} seconds\")\n    \n    # Display the results\n    print(\"\\nSentiment Analysis Results:\")\n    print(df[['review', 'sentiment']].head(10))\n    \n    # Save the results to a new CSV file\n    output_file = 'drug_reviews_with_sentiment.csv'\n    df.to_csv(output_file, index=False)\n    print(f\"\\nResults saved to {output_file}\")\n    \n    # Calculate some statistics\n    sentiment_counts = df['sentiment'].value_counts()\n    print(\"\\nSentiment Distribution:\")\n    print(sentiment_counts)\n    \n    # Calculate percentage of positive reviews\n    if len(df) > 0:\n        positive_percentage = (sentiment_counts.get('Positive', 0) / len(df)) * 100\n        print(f\"\\nPercentage of Positive Reviews: {positive_percentage:.2f}%\")\n    \nexcept FileNotFoundError:\n    print(\"Error: drug_reviews.csv file not found. Please check the file path.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {str(e)}\")",
      "outputs": [],
      "metadata": {
        "id": "8fe1daf7-35a9-479d-85f2-31172d05ed16",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "## 2. Now with streaming",
      "outputs": [],
      "metadata": {
        "id": "f3bcd89b-b054-4b73-a6d9-59df5de9e56d"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Define a streaming function to demonstrate real-time responses from the LLM\n# This builds on the previous section by showing how to stream responses\n# rather than waiting for the complete response\n\nimport time\nfrom ai21 import AI21Client\nfrom ai21.models.chat import ChatMessage\n\n# Define the system and user messages (similar to previous example)\nsystem = \"You're a data feature enrichment engineer.\"\nmessages = [\n    ChatMessage(content=system, role=\"system\"),\n    ChatMessage(content=\"Explain how to enrich a CSV with reviews with sentiment data in 5 steps.\", role=\"user\"),\n]\n\n# Start timing the request\nstart_time = time.time()\n\n# Make the streaming API call\n# The stream=True parameter enables streaming responses\nprint(\"Streaming Response:\")\nprint(\"-\" * 50)\n\n# Initialize variables to track token usage\ntotal_tokens = 0\nresponse_content = \"\"\n\n# Stream the response\nfor chunk in client.chat.completions.create(\n    messages=messages,\n    model=model,\n    stream=True,\n):\n    # Extract and print each chunk of the response as it arrives\n    if chunk.choices and chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        print(content, end=\"\", flush=True)\n        response_content += content\n        total_tokens += 1  # Approximate token count\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"\\n\" + \"-\" * 50)\n\n# Display metadata about the streaming response\nprint(\"\\nStreaming Response Metadata:\")\nprint(f\"Request Time: {elapsed_time:.2f} seconds\")\nprint(f\"Approximate Completion Tokens: {total_tokens}\")\nprint(f\"Model: {model}\")\n\n# Compare streaming vs non-streaming\nprint(\"\\nAdvantages of Streaming:\")\nprint(\"1. Immediate feedback to users\")\nprint(\"2. Better perceived performance\")\nprint(\"3. Ability to process partial responses\")\nprint(\"4. Can cancel long-running requests early\")",
      "outputs": [],
      "metadata": {
        "id": "ded5310a-4f00-4dcf-b6aa-b0d6eae654f1",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "### Loop through rows in drug_reviews.csv to extract \"Positive, Negative\" given the text in \"review\" column while streaming each one",
      "outputs": [],
      "metadata": {
        "id": "870cd212-015b-4d14-a1ea-5b62e272d616"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Import necessary libraries for streaming sentiment analysis\nimport pandas as pd\nimport time\nfrom ai21 import AI21Client\nfrom ai21.models.chat import ChatMessage\nfrom tqdm import tqdm\n\n# Load the drug reviews dataset\ntry:\n    # Load the dataset (limited to 20 rows for demonstration)\n    print(\"Loading drug reviews dataset...\")\n    df = pd.read_csv('drug_reviews.csv', nrows=20)\n    print(f\"Loaded {len(df)} rows for processing\")\n    \n    # Create a new column to store sentiment analysis results\n    df['sentiment'] = \"\"\n    \n    # Set up the AI21 client (using the client already initialized above)\n    client = AI21Client()\n    model = \"jamba-mini-1.6-2025-03\"\n    \n    # Define the system prompt for sentiment analysis\n    system_prompt = \"You are a sentiment analysis expert. Analyze the drug review and respond with 'Positive' or 'Negative' with reasoning.\"\n    \n    # Process each review and extract sentiment with streaming\n    print(\"\\nProcessing reviews with streaming sentiment analysis...\")\n    \n    # Track time for performance analysis\n    start_time = time.time()\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        # Create messages for the API call\n        messages = [\n            ChatMessage(content=system_prompt, role=\"system\"),\n            ChatMessage(content=f\"Analyze the sentiment of this drug review: {row['review']}\", role=\"user\"),\n        ]\n        \n        # Make the streaming API call\n        try:\n            print(f\"\\nAnalyzing review {idx+1}/{len(df)}: \", end=\"\", flush=True)\n            \n            # Initialize variables to collect the streamed response\n            full_response = \"\"\n            \n            # Stream the response\n            for chunk in client.chat.completions.create(\n                messages=messages,\n                model=model,\n                stream=True,\n            ):\n                # Extract and print each chunk of the response as it arrives\n                if chunk.choices and chunk.choices[0].delta.content:\n                    content = chunk.choices[0].delta.content\n                    print(content, end=\"\", flush=True)\n                    full_response += content\n            \n            # Store the sentiment in the dataframe\n            df.at[idx, 'sentiment'] = full_response.strip()\n            \n            # Add a small delay to avoid rate limiting (if needed)\n            time.sleep(0.1)\n            \n        except Exception as e:\n            print(f\"\\nError processing row {idx}: {str(e)}\")\n            df.at[idx, 'sentiment'] = \"Error\"\n    \n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n    print(f\"\\n\\nProcessing completed in {elapsed_time:.2f} seconds\")\n    \n    # Display the results\n    print(\"\\nSentiment Analysis Results:\")\n    print(df[['review', 'sentiment']].head(10))\n    \n    # Save the results to a new CSV file\n    output_file = 'drug_reviews_with_streaming_sentiment.csv'\n    df.to_csv(output_file, index=False)\n    print(f\"\\nResults saved to {output_file}\")\n    \n    # Calculate some statistics\n    sentiment_counts = df['sentiment'].value_counts()\n    print(\"\\nSentiment Distribution:\")\n    print(sentiment_counts)\n    \n    # Calculate percentage of positive reviews\n    if len(df) > 0:\n        positive_count = sum(1 for s in df['sentiment'] if s.lower().startswith('positive'))\n        positive_percentage = (positive_count / len(df)) * 100\n        print(f\"\\nPercentage of Positive Reviews: {positive_percentage:.2f}%\")\n    \nexcept FileNotFoundError:\n    print(\"Error: drug_reviews.csv file not found. Please check the file path.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {str(e)}\")",
      "outputs": [],
      "metadata": {
        "id": "a93cba7c-ee2c-46af-8a6e-1e118355506f",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "## 3. Now with Concurrent Calls with Async Client",
      "outputs": [],
      "metadata": {
        "id": "fd9dc668-44e8-44c3-b7f4-389d47072ea1"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Import necessary libraries for async operations\nimport asyncio\nimport time\nfrom ai21 import AsyncAI21Client\nfrom ai21.models.chat import ChatMessage\n\n# Define a function to make async API calls to AI21\nasync def make_async_call(client, messages, model_name):\n    start_time = time.time()\n    \n    # Make the async API call\n    response = await client.chat.completions.create(\n        messages=messages,\n        model=model_name,\n    )\n    \n    elapsed_time = time.time() - start_time\n    \n    # Return both the response and timing information\n    return response, elapsed_time\n\n# Set up the async client using environment variables (already loaded above)\nasync_client = AsyncAI21Client()\n\n# Define different prompts to demonstrate parallel processing\nsystem_prompt1 = \"You're a data feature enrichment engineer.\"\nsystem_prompt2 = \"You're a customer support specialist.\"\nsystem_prompt3 = \"You're a technical documentation writer.\"\n\n# Create message sets for each prompt\nmessages1 = [\n    ChatMessage(content=system_prompt1, role=\"system\"),\n    ChatMessage(content=\"Hello, I need help with enriching CSVs.\", role=\"user\"),\n]\nmessages2 = [\n    ChatMessage(content=system_prompt2, role=\"system\"),\n    ChatMessage(content=\"Hello, I need help with enriching CSVs.\", role=\"user\"),\n]\nmessages3 = [\n    ChatMessage(content=system_prompt3, role=\"system\"),\n    ChatMessage(content=\"Hello, I need help with enriching CSVs.\", role=\"user\"),\n]\n\n# Define the main async function to run multiple calls concurrently\nasync def run_concurrent_calls():\n    # Start timing the entire concurrent operation\n    total_start_time = time.time()\n    \n    # Run all three API calls concurrently\n    tasks = [\n        make_async_call(async_client, messages1, model),\n        make_async_call(async_client, messages2, model),\n        make_async_call(async_client, messages3, model),\n    ]\n    \n    # Gather results from all tasks\n    results = await asyncio.gather(*tasks)\n    \n    total_elapsed_time = time.time() - total_start_time\n    \n    # Print results from each call\n    for i, (response, call_time) in enumerate(results, 1):\n        print(f\"\\nResponse {i}:\")\n        print(\"-\" * 50)\n        print(f\"Role: {response.choices[0].message.role}\")\n        print(f\"Content: {response.choices[0].message.content}\")\n        print(f\"Request Time: {call_time:.2f} seconds\")\n        \n        # Print usage information\n        if hasattr(response, 'usage'):\n            usage = response.usage\n            print(f\"Usage - Prompt Tokens: {usage.prompt_tokens}\")\n            print(f\"Usage - Completion Tokens: {usage.completion_tokens}\")\n            print(f\"Usage - Total Tokens: {usage.total_tokens}\")\n    \n    print(\"\\nConcurrent Execution Summary:\")\n    print(f\"Total time for all 3 requests: {total_elapsed_time:.2f} seconds\")\n    print(f\"Average time per request if sequential: {sum(time for _, time in results)/len(results):.2f} seconds\")\n    print(f\"Time saved with async: {(sum(time for _, time in results) - total_elapsed_time):.2f} seconds\")\n\n# Run the async function\nawait run_concurrent_calls()",
      "outputs": [],
      "metadata": {
        "id": "bd59ce22-288e-448c-a2cf-d8bf402d01f9",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "### Concurrently extract \"Positive, Negative\" with reasoning given the text in \"review\" column in drug_reviews.csv",
      "outputs": [],
      "metadata": {
        "id": "ba210f7b-c7d9-4343-baa7-51a937bc95a0"
      },
      "cell_type": "markdown",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "# Import necessary libraries for async sentiment analysis\nimport pandas as pd\nimport asyncio\nimport time\nfrom ai21 import AsyncAI21Client\nfrom ai21.models.chat import ChatMessage\nfrom tqdm.asyncio import tqdm_asyncio\n\n# Define an async function to process a single review\nasync def process_review(client, model, review, idx, total):\n    \"\"\"\n    Process a single drug review for sentiment analysis using async API calls.\n    \n    Args:\n        client: The AsyncAI21Client instance\n        model: The AI model to use\n        review: The text of the review to analyze\n        idx: The index of the current review\n        total: The total number of reviews to process\n        \n    Returns:\n        The sentiment analysis result as a string\n    \"\"\"\n    # Define the system prompt for sentiment analysis\n    system_prompt = \"You are a sentiment analysis expert. Analyze the drug review and respond 'Positive' or 'Negative' with reasoning.\"\n    \n    # Create messages for the API call\n    messages = [\n        ChatMessage(content=system_prompt, role=\"system\"),\n        ChatMessage(content=f\"Analyze the sentiment of this drug review: {review}\", role=\"user\"),\n    ]\n    \n    # Make the API call with streaming\n    try:\n        print(f\"\\nAnalyzing review {idx+1}/{total}: \", end=\"\", flush=True)\n        \n        # Initialize variable to collect the streamed response\n        full_response = \"\"\n        \n        # Stream the response\n        async for chunk in await client.chat.completions.create(\n            messages=messages,\n            model=model,\n            stream=True,\n        ):\n            # Extract and print each chunk of the response as it arrives\n            if chunk.choices and chunk.choices[0].delta.content:\n                content = chunk.choices[0].delta.content\n                print(content, end=\"\", flush=True)\n                full_response += content\n        \n        # Return the sentiment analysis result\n        return full_response.strip()\n        \n    except Exception as e:\n        print(f\"\\nError processing review {idx+1}: {str(e)}\")\n        return \"Error\"\n\n# Define the main async function to process all reviews concurrently\nasync def process_reviews_concurrently(df, batch_size=5):\n    \"\"\"\n    Process multiple reviews concurrently in batches using async API calls.\n    \n    Args:\n        df: DataFrame containing the reviews to process\n        batch_size: Number of reviews to process concurrently in each batch\n        \n    Returns:\n        Tuple of (processed DataFrame, elapsed time)\n    \"\"\"\n    # Set up the async client\n    async_client = AsyncAI21Client()\n    model = \"jamba-mini-1.6-2025-03\"\n    \n    # Create a new column to store sentiment analysis results\n    df['sentiment'] = \"\"\n    \n    # Track time for performance analysis\n    start_time = time.time()\n    \n    # Process reviews in batches to avoid overwhelming the API\n    for i in range(0, len(df), batch_size):\n        batch_df = df.iloc[i:i+batch_size]\n        \n        # Create tasks for concurrent processing\n        tasks = [\n            process_review(\n                async_client, \n                model, \n                row['review'], \n                idx, \n                len(df)\n            ) for idx, row in batch_df.iterrows()\n        ]\n        \n        # Process the batch concurrently and get results\n        results = await tqdm_asyncio.gather(*tasks)\n        \n        # Store results in the dataframe\n        for (idx, _), sentiment in zip(batch_df.iterrows(), results):\n            df.at[idx, 'sentiment'] = sentiment\n    \n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n    return df, elapsed_time\n\n# Main execution function\nasync def main():\n    \"\"\"\n    Main function to load data, process reviews, and save results.\n    \"\"\"\n    try:\n        # Load the drug reviews dataset (limited to 20 rows for demonstration)\n        print(\"Loading drug reviews dataset...\")\n        df = pd.read_csv('drug_reviews.csv', nrows=20)\n        print(f\"Loaded {len(df)} rows for processing\")\n        \n        # Process the reviews concurrently\n        print(\"\\nProcessing reviews with concurrent streaming sentiment analysis...\")\n        result_df, elapsed_time = await process_reviews_concurrently(df)\n        \n        print(f\"\\n\\nProcessing completed in {elapsed_time:.2f} seconds\")\n        \n        # Display the results\n        print(\"\\nSentiment Analysis Results:\")\n        print(result_df[['review', 'sentiment']].head(10))\n        \n        # Save the results to a new CSV file\n        output_file = 'drug_reviews_with_concurrent_sentiment.csv'\n        result_df.to_csv(output_file, index=False)\n        print(f\"\\nResults saved to {output_file}\")\n        \n        # Calculate some statistics\n        positive_count = sum(1 for s in result_df['sentiment'] if 'positive' in s.lower())\n        positive_percentage = (positive_count / len(result_df)) * 100\n        print(f\"\\nPercentage of Positive Reviews: {positive_percentage:.2f}%\")\n        \n        # Compare with previous non-concurrent approach\n        print(\"\\nPerformance Comparison:\")\n        print(f\"Concurrent processing time: {elapsed_time:.2f} seconds\")\n        print(\"Non-concurrent processing time from previous cell: ~14 seconds\")\n        print(f\"Speed improvement: ~{14/elapsed_time:.1f}x faster with concurrent processing\")\n        \n    except FileNotFoundError:\n        print(\"Error: drug_reviews.csv file not found. Please check the file path.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n\n# Run the main async function\nawait main()",
      "outputs": [],
      "metadata": {
        "id": "729a0c46-6b82-4dc5-ab84-c5469a0d731b",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    },
    {
      "source": "",
      "outputs": [],
      "metadata": {
        "id": "f3adb9c9-42be-45ad-b8a9-67b51b7c7cfc",
        "trusted": true
      },
      "cell_type": "code",
      "isExecuting": false,
      "stdinRequest": null,
      "executionState": "idle",
      "execution_count": null,
      "executionEndTime": null,
      "executionStartTime": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}